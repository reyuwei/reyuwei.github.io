# GAN <!-- omit in toc -->

###### Tue Aug 6 14:07:09 CST 2019

- [What I want to know](#what-i-want-to-know)
- [Materials](#materials)
- [Notes](#notes)
  - [Basics](#basics)
    - [优化方程](#%e4%bc%98%e5%8c%96%e6%96%b9%e7%a8%8b)
    - [网络的两个部分](#%e7%bd%91%e7%bb%9c%e7%9a%84%e4%b8%a4%e4%b8%aa%e9%83%a8%e5%88%86)
    - [Training](#training)
  - [High level understanding](#high-level-understanding)
    - [GANs, Encoder-decoder, Autoencoders and VAEs](#gans-encoder-decoder-autoencoders-and-vaes)
  - [Applications](#applications)
  - [Notes on NIPS tutorial](#notes-on-nips-tutorial)
    - [RoadMap](#roadmap)

## What I want to know
- What is GAN?
- Why does it work?
- How does learn the data distribution?
- How can it help my project?

## Materials

[Thesis] [Learning to Synthesize and Manipulate Natural Images](http://people.csail.mit.edu/junyanz/pdf/thesis_lowres.pdf) (*SIG18 best PhD thesis*)

[Video] [Introduction to GANs, NIPS 2016, Ian Goodfellow, OpenAI](https://www.youtube.com/watch?v=9JpdAg6uMXs) (*30mins*)

<span id='nips2h'></span>
[Video] [Ian Goodfellow: Generative Adversarial Networks (NIPS 2016 tutorial)](https://www.youtube.com/watch?v=HGYYEUSm-0Q) (*2hrs*)

[WebPage] [MSRA: 到底什么是生成式对抗网络GAN？](https://www.msra.cn/zh-cn/news/features/gan-20170511)

<span id="web2"></span>
[WebPage] [A Beginner's Guide to Generative Adversarial Networks (GANs)](https://skymind.ai/wiki/generative-adversarial-network-gan)
## Notes

### Basics 
GAN网络有一个生成器$\mathbf{G}$和一个判别器$\mathbf{D}$，生成器的目标是为了学习数据分布，判别器的目标是判定输入的数据是否符合真实分布。

#### 优化方程
$min_\mathbf{G}max_\mathbf{D}\{E_{x\sim{}P_r}[log\mathbf{D}(x)]+E_{x\sim{}P_g}[log(1-\mathbf{D}(x))]\}$

#### 网络的两个部分

- input noise $z$ -> $\mathbf{G}$ -> x sampled from $\mathbf{G}$, $x=\mathbf{G}(z)$ -> $\mathbf{D}$ -> $\mathbf{G}$ tries to make $\mathbf{D}(x)\rightarrow1$, $\mathbf{D}$ tries to make $\mathbf{D}(x)\rightarrow0$

- x sampled from data -> $\mathbf{D}$ -> $D(x)\rightarrow1$

#### Training
最直观的处理办法就是分别对D和g进行交互迭代，固定g，优化D，一段时间后，固定D再优化g，直到过程收敛。

### High level understanding
Basically, GAN consists of a generator and a discriminator. The generator takes a feature vector, which in this case is random noise, and outputs a sample that mimics the data distribution. The discriminator takes a sample either from the original data or generated by generator, its job is to successfully distinguish real ones from fake ones.

As shown in the following figure, discriminator is supervised by ground truth label, while the generator is supervised by discriminator.
![](../images/gan_schema.png)

Looking at it separately, the discriminator does a standard binary classification task, real or fake. The network returns a probability, a number between 0 and 1, with 1 representing a prediction of authenticity and 0 representing fake.

The generator takes a feature and expands it to a data sample. It is the opposite process of discriminator. The goal of a generator is to learn the data distribution, how to tell if you have successfully learned the distribution? The generator generates data from random noise, if the discriminator thinks it is real, the generator can be said to have learned the real distribution of the dataset. 

> The question a generative algorithm tries to answer is: Assuming this email is spam, how likely are these features (words it contains)? While discriminative models care about the relation between $y$ (label) and $x$ (data), generative models care about “how you get $x$.” They allow you to capture $p(x\vert y)$, the probability of $x$ given $y$, or the probability of features given a label or category.

The discriminator, on the other hand, captures $p(y\vert x)$, the probability of $y$ given $x$. Which is given an email (words it contains), how likely the email is spam.

#### GANs, Encoder-decoder, Autoencoders and VAEs

*The generator in GAN serves the similar function as a decoder in Encoder-decoder network??? What are the differences.*

> [*[quote]*](#web2)
> You can bucket generative algorithms into one of three types:
> - Given a label, they predict the associated features (Naive Bayes)
> - Given a hidden representation, they predict the associated features (VAE, GAN)
> - Given some of the features, they predict the rest (inpainting, imputation)

### Applications
- Same domain
  - super resolution
  - image filling / repairing
- Cross domain transfer
  - 2d to 3d
  - text to image
  - picture style transfer
- Learn joint distribution
  - learn attributes from images

### Notes on [NIPS tutorial](#nips2h)
#### RoadMap
- Why study generative modeling?
- How do generative models work? How do GANs compare to others?
- How do GANs work?
- Tips and tricks
- Research frontiers
- Combining GANs with other methods